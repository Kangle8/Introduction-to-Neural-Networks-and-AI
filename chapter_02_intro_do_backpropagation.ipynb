{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e05635-45e7-4d10-9dd1-00ec79213906",
   "metadata": {
    "id": "80e05635-45e7-4d10-9dd1-00ec79213906"
   },
   "source": [
    "# The StatQuest Illustrated Guide to Neural Networks and AI\n",
    "## Chapter 2 - Build and train a very simple neural network with backpropagation\n",
    "\n",
    "Copyright 2024, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3466e-96aa-47bb-a7af-b8e9a93d3d5d",
   "metadata": {
    "id": "61a3466e-96aa-47bb-a7af-b8e9a93d3d5d"
   },
   "source": [
    "In this notebook, we will build, and train the very simple neural network featured in Chapter 2 of **[The StatQuest Illustrated Guide to Neural Networks and AI](https://www.amazon.com/dp/B0DRS71QVQ)**. We'll start with the untrained Weights and Biases, as seen in the figure below...\n",
    "\n",
    "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_2_train_all.png?raw=1\" alt=\"an untrained neural network\" style=\"width: 800px;\">\n",
    "\n",
    "...and then train them with Backpropagation to, hopefully, get the trained Weights and Biases seen in the figure below.\n",
    "\n",
    "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_1_pre_trained_nn_labeled.png?raw=1\" alt=\"a trained neural network\" style=\"width: 800px;\">\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Code a neural network with untrained wieghts and biases](#untrained)** This will show the basic structure of a class that inherits from `LightningModule` to build a neural network.\n",
    "\n",
    "- **[Train the weights and biases in our neural network](#train)** This will show how to train the weights and biases in a simple neural network.\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you already know the basics of coding in **Python** and have read the first two chapters in **The StatQuest Illustrated Guide to Neural Networks and AI**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79033cc7-bcd6-409c-842f-ed7bac55c687",
   "metadata": {
    "id": "79033cc7-bcd6-409c-842f-ed7bac55c687"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a97a7-ba0d-449a-8c1f-551d468e6038",
   "metadata": {
    "id": "252a97a7-ba0d-449a-8c1f-551d468e6038"
   },
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b9885b-59a2-4148-ad19-0c822471a360",
   "metadata": {
    "id": "c7b9885b-59a2-4148-ad19-0c822471a360"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## NOTE: If you **don't** need to install anything, you can comment out the\n",
    "##       next line.\n",
    "##\n",
    "##       If you **do** need to install something, just know that you may need to\n",
    "##       restart your session for python to find the new module(s).\n",
    "##\n",
    "##       To restart your session:\n",
    "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
    "##         \"Restart Session\" from the pulldown menu\n",
    "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
    "##         \"Restart Kernel\" from the pulldown menu\n",
    "##\n",
    "##       Also, installing can take a few minutes, so go get yourself a snack!\n",
    "!pip install lightning seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd0881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhukangle/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "tokenizers>=0.11.1,!=0.11.3,<0.14 is required for a normal functioning of this module, but found tokenizers==0.21.2.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(L\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/callbacks/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_finder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSizeFinder\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/callbacks/batch_size_finder.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_scaling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _scale_batch_size\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MisconfigurationException, _TunerExitException\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/callbacks/callback.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEP_OUTPUT\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCallback\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Abstract base class used to build new callbacks.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Subclass this class and override any of the relevant hooks\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/utilities/types.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRScheduler, ReduceLROnPlateau\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotRequired, Required\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProcessGroup\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchmetrics/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(scipy\u001b[38;5;241m.\u001b[39msignal, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhamming\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     35\u001b[0m         scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mhamming \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mwindows\u001b[38;5;241m.\u001b[39mhamming\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maggregation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     CatMetric,\n\u001b[1;32m     40\u001b[0m     MaxMetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     SumMetric,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _PermutationInvariantTraining \u001b[38;5;28;01mas\u001b[39;00m PermutationInvariantTraining  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchmetrics/functional/__init__.py:128\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _retrieval_recall \u001b[38;5;28;01mas\u001b[39;00m retrieval_recall\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _retrieval_reciprocal_rank \u001b[38;5;28;01mas\u001b[39;00m retrieval_reciprocal_rank\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _bleu_score \u001b[38;5;28;01mas\u001b[39;00m bleu_score\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _char_error_rate \u001b[38;5;28;01mas\u001b[39;00m char_error_rate\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _chrf_score \u001b[38;5;28;01mas\u001b[39;00m chrf_score\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchmetrics/functional/text/__init__.py:50\u001b[0m\n\u001b[1;32m     31\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_error_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_information_preserved\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m ]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TRANSFORMERS_GREATER_EQUAL_4_4:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bert_score\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfolm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infolm\n\u001b[1;32m     53\u001b[0m     __all__ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfolm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchmetrics/functional/text/bert.py:56\u001b[0m\n\u001b[1;32m     53\u001b[0m _DEFAULT_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TRANSFORMERS_GREATER_EQUAL_4_4:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_model_for_bert_score\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Download intensive operations.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     logging,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/dependency_versions_check.py:57\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install transformers -U or pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 111\u001b[0m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to compare versions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: need=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwant_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is unusual. Consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reinstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: tokenizers>=0.11.1,!=0.11.3,<0.14 is required for a normal functioning of this module, but found tokenizers==0.21.2.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "print(L.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c818152-6ca2-4ee0-9168-f77c9b4cac1c",
   "metadata": {
    "id": "8c818152-6ca2-4ee0-9168-f77c9b4cac1c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD \u001b[38;5;66;03m# SGD is short of Stochastic Gradient Descent, but\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                             \u001b[38;5;66;03m# the way we'll use it, passing in all of the training\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                             \u001b[38;5;66;03m# data at once instead of passing it random subsets,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                             \u001b[38;5;66;03m# it will act just like plain old Gradient Descent.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m \u001b[38;5;66;03m## Lightning makes it easier to write, optimize and scale our code\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader \u001b[38;5;66;03m## We'll store our data in DataLoaders\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \u001b[38;5;66;03m## matplotlib allows us to draw graphs.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightning'"
     ]
    }
   ],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us relu()\n",
    "from torch.optim import SGD # SGD is short of Stochastic Gradient Descent, but\n",
    "                            # the way we'll use it, passing in all of the training\n",
    "                            # data at once instead of passing it random subsets,\n",
    "                            # it will act just like plain old Gradient Descent.\n",
    "\n",
    "import lightning as L ## Lightning makes it easier to write, optimize and scale our code\n",
    "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
    "\n",
    "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
    "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.\n",
    "\n",
    "## NOTE: If you get an error running this block of code, it is probably\n",
    "##       because you installed a new package earlier and forgot to\n",
    "##       restart your session for python to find the new module(s).\n",
    "##\n",
    "##       To restart your session:\n",
    "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
    "##         \"Restart Session\" from the pulldown menu\n",
    "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
    "##         \"Restart Kernel\" from the pulldown menu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06585c7-4310-4f59-8c22-55f53a17e905",
   "metadata": {
    "id": "c06585c7-4310-4f59-8c22-55f53a17e905"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca5fb5-e879-4453-9e86-dbc37c158afa",
   "metadata": {
    "id": "b4ca5fb5-e879-4453-9e86-dbc37c158afa"
   },
   "source": [
    "# Create the Training Dataset\n",
    "\n",
    "In Chapter 2, we had a very simple dataset that consisted of three points, as seen in the figure below.\n",
    "\n",
    "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_2_training_data.png?raw=1\" alt=\"a simple dataset for training\" style=\"width: 800px;\">\n",
    "\n",
    "Although it's not required, we're going to put our training data into a `DataLoader`. `DataLoaders` are easy to make and they offer a lot of cool features. For example, if we had a large dataset, a `DataLoader` gives us a super easy way to access the data in batches instead of all at once. This is critical when we have more data than RAM to store it in. A DataLoader can also shuffle the data for us each epoch and makes it easy to only use a fraction of the data if we want to do a quick and rough training for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49840acf-ddb6-4577-8401-de872ab62c32",
   "metadata": {
    "id": "49840acf-ddb6-4577-8401-de872ab62c32"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m training_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m## Now let's package everything up into a DataLoader...\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m(training_inputs, training_labels)\n\u001b[1;32m     11\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(training_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
     ]
    }
   ],
   "source": [
    "## The inputs are the x-axis coordinates for each data point\n",
    "## These values represent different doses\n",
    "training_inputs = torch.tensor([0.0, 0.5, 1.0])\n",
    "\n",
    "## The labels are the y-axis coordinates for each data point\n",
    "## These values represent the effectiveness\n",
    "training_labels = torch.tensor([0.0, 1.0, 0.0])\n",
    "\n",
    "## Now let's package everything up into a DataLoader...\n",
    "training_dataset = TensorDataset(training_inputs, training_labels)\n",
    "dataloader = DataLoader(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d183574-134b-4be3-9fd5-186a3a946d9b",
   "metadata": {
    "id": "7d183574-134b-4be3-9fd5-186a3a946d9b"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adfb2eb-0ad7-426d-9946-6a7185737647",
   "metadata": {
    "id": "7adfb2eb-0ad7-426d-9946-6a7185737647"
   },
   "source": [
    "# Create a Neural Network with Trainable Weights and Biases\n",
    "<a id=\"create\"></a>\n",
    "\n",
    "Now we'll build a neural network that has trainable Weights and Biases. For this, we'll use `L.LightningModule`, which has everything `nn.Module` has, plus we can define the optimizer we want to use as well as tell PyTorch how each training step should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881afee-1cd9-4440-8ffc-1f86b321c539",
   "metadata": {
    "id": "4881afee-1cd9-4440-8ffc-1f86b321c539"
   },
   "outputs": [],
   "source": [
    "class myNN(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        ## Create all of the weights and biases for the network.\n",
    "        ## However, this time they are initialized with random values.\n",
    "        ## We are also wrapping the tensors up in nn.Parameter() objects.\n",
    "        ## PyTorch will only optimize parameters. There are a lot of\n",
    "        ## different ways to create parameters, and we'll see those\n",
    "        ## in later examples, but nn.Parameter() is the most basic.\n",
    "        self.w1 = nn.Parameter(torch.tensor(0.06))\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        self.w2 = nn.Parameter(torch.tensor(3.49))\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        self.w3 = nn.Parameter(torch.tensor(-4.11))\n",
    "        self.w4 = nn.Parameter(torch.tensor(2.74))\n",
    "\n",
    "        self.loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        ## The forward() method is identical to what we used in Chapter 1.\n",
    "\n",
    "        top_x_axis_values = (input_values * self.w1) + self.b1\n",
    "        bottom_x_axis_values = (input_values * self.w2) + self.b2\n",
    "\n",
    "        top_y_axis_values = F.relu(top_x_axis_values)\n",
    "        bottom_y_axis_values = F.relu(bottom_x_axis_values)\n",
    "\n",
    "        output_values = (top_y_axis_values * self.w3) + (bottom_y_axis_values * self.w4)\n",
    "\n",
    "        return output_values\n",
    "\n",
    "\n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return SGD(self.parameters(), lr=0.01)\n",
    "        ## NOTE: PyTorch doesn't have a Gradient Descent optimizer, just a\n",
    "        ## Stochastic Gradient Descent (SGD) optimizer. However, since we\n",
    "        ## are running all 3 doses through the NN each time, rather than a\n",
    "        ## random subset, we are essentially doing Gradient Descent instead of\n",
    "        ## SGD.\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        ## NOTE: When training_step() is called it calculates the loss with the code below...\n",
    "        inputs, labels = batch # collect input\n",
    "        outputs = self.forward(inputs) # run input through the neural network\n",
    "        loss = self.loss(outputs, labels) ## the `loss` quantifies the difference between\n",
    "                                          ## the observed drug effectiveness in `labels`\n",
    "                                          ## and the outputs created by the neural network\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36d865-fcab-435d-9b1d-484a3ed8cd7a",
   "metadata": {
    "id": "ff36d865-fcab-435d-9b1d-484a3ed8cd7a"
   },
   "outputs": [],
   "source": [
    "model = myNN() # First, make model from the class\n",
    "\n",
    "## Now print out the name and value for each named parameter\n",
    "## parameter in the model. Remember parameters are variables,\n",
    "## like Weights and Biases, that we can train.\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1842d-b97c-4b8f-91c9-6bc6884e1dfa",
   "metadata": {
    "id": "9bd1842d-b97c-4b8f-91c9-6bc6884e1dfa"
   },
   "outputs": [],
   "source": [
    "## now run different doses through the neural network.\n",
    "output_values = model(training_inputs)\n",
    "torch.round(output_values, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0a5a5-47f0-4e45-be2b-a7cf10756e29",
   "metadata": {
    "id": "e0c0a5a5-47f0-4e45-be2b-a7cf10756e29"
   },
   "source": [
    "# BAM!\n",
    "\n",
    "We successfully ran the doses from the training data through the model. However, the output from the model is way different than we expect (we expected 0.0, 1.0, and 0.0). So let's draw a picture of the bent shape that the model uses to make predictions and compare that to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901253af-43f6-40dc-97af-175c36e5290e",
   "metadata": {
    "id": "901253af-43f6-40dc-97af-175c36e5290e"
   },
   "outputs": [],
   "source": [
    "## Create the different doses we want to run through the neural network.\n",
    "## torch.linspace() creates the sequence of numbers between, and including, 0 and 1.\n",
    "input_doses = torch.linspace(start=0, end=1, steps=11)\n",
    "\n",
    "# now print out the doses to make sure they are what we expect...\n",
    "input_doses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cbadf-86ec-4f11-af2c-dfece12f197a",
   "metadata": {
    "id": "c84cbadf-86ec-4f11-af2c-dfece12f197a"
   },
   "outputs": [],
   "source": [
    "output_values = model(input_doses)\n",
    "output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc01f1-65d4-4fb0-b428-6730f64d829a",
   "metadata": {
    "id": "f3bc01f1-65d4-4fb0-b428-6730f64d829a"
   },
   "outputs": [],
   "source": [
    "## Now draw a graph that shows how well, or poorly, the model\n",
    "## predicts the training data. At this point, since the\n",
    "## model is untrained, there should be a big difference between\n",
    "## the model's output and the training data.\n",
    "\n",
    "## First, set the style for seaborn so that the graph looks cool.\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "## First, draw the individual output points\n",
    "sns.scatterplot(x=input_doses,\n",
    "                y=output_values.detach().numpy(),\n",
    "                color='green',\n",
    "                s=200)\n",
    "\n",
    "## Now connect those points with a line\n",
    "sns.lineplot(x=input_doses,\n",
    "             y=output_values.detach().numpy(), ## NOTE: We call .detatch() because...\n",
    "             color='green',\n",
    "             linewidth=2.5)\n",
    "\n",
    "## Add the values in the training dataset\n",
    "sns.scatterplot(x=training_inputs,\n",
    "                y=training_labels,\n",
    "                color='orange',\n",
    "                s=200)\n",
    "\n",
    "## now label the y- and x-axes.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b10eaf-4fae-46bc-af6c-f05ebdada746",
   "metadata": {
    "id": "89b10eaf-4fae-46bc-af6c-f05ebdada746"
   },
   "source": [
    "# DOUBLE BAM!!\n",
    "\n",
    "Now that we see how badly the bent shape fits the training data, let's train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab161cb5-f837-42b1-b47b-1a4af10b2169",
   "metadata": {
    "id": "ab161cb5-f837-42b1-b47b-1a4af10b2169"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59123d80-2a03-4f66-9a41-f3e85b040ddd",
   "metadata": {
    "id": "59123d80-2a03-4f66-9a41-f3e85b040ddd"
   },
   "source": [
    "# Training the Weights and Biases in the Neural Network\n",
    "\n",
    "Training consists of creating a **Lightning Trainer** with `L.Trainer()` and then calling the `fit()` method on the our model with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b9bc6-39ac-4fb1-aabd-355f22f54677",
   "metadata": {
    "id": "033b9bc6-39ac-4fb1-aabd-355f22f54677"
   },
   "outputs": [],
   "source": [
    "model = myNN()\n",
    "## Now train the model...\n",
    "trainer = L.Trainer(max_epochs=500, # how many times to go through the training data\n",
    "                    logger=False,\n",
    "                    enable_checkpointing=False,\n",
    "                    enable_progress_bar=False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b0cf3-9541-4d59-853f-63202fbad7ae",
   "metadata": {
    "id": "146b0cf3-9541-4d59-853f-63202fbad7ae"
   },
   "outputs": [],
   "source": [
    "## Now that we've trained the model, let's print out the\n",
    "## new values for each Weight and Bias.\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1935b5-4256-49c9-9117-8aab8ad382b3",
   "metadata": {
    "id": "bd1935b5-4256-49c9-9117-8aab8ad382b3"
   },
   "source": [
    "Lastly, let's draw a graph of the bent shape that the model is using for predictions and compare it to the training data. In theory, the bent shape should fit the data much better now that we have optimized the Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11539eec-29cc-4efb-ab2e-ddf82ba360c2",
   "metadata": {
    "id": "11539eec-29cc-4efb-ab2e-ddf82ba360c2"
   },
   "outputs": [],
   "source": [
    "## now run the different doses through the neural network.\n",
    "output_values = model(input_doses)\n",
    "torch.round(output_values, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b2d67-8621-4be3-937e-e1fc980475a9",
   "metadata": {
    "id": "186b2d67-8621-4be3-937e-e1fc980475a9"
   },
   "outputs": [],
   "source": [
    "## Now draw a graph that shows how well, or poorly, the model\n",
    "## predicts the training data. At this point, since we just\n",
    "## trained th model, the training data should overlap the\n",
    "## model's output\n",
    "\n",
    "## First, set the style for seaborn so that the graph looks cool.\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "## First, draw the individual output points\n",
    "sns.scatterplot(x=input_doses,\n",
    "                y=output_values.detach().numpy(),\n",
    "                color='green',\n",
    "                s=200)\n",
    "\n",
    "## Now connect those points with a line\n",
    "sns.lineplot(x=input_doses,\n",
    "             y=output_values.detach().numpy(), ## NOTE: We call .detatch() because...\n",
    "             color='green',\n",
    "             linewidth=2.5)\n",
    "\n",
    "## Add the values in the training dataset\n",
    "sns.scatterplot(x=training_inputs,\n",
    "                y=training_labels,\n",
    "                color='orange',\n",
    "                s=200)\n",
    "\n",
    "## now label the y- and x-axes.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a835c4-5041-4974-987f-9c17b8396908",
   "metadata": {
    "id": "b6a835c4-5041-4974-987f-9c17b8396908"
   },
   "source": [
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
